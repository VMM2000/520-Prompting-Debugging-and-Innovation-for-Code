{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2886d4db",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ccf45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytest pytest-cov pytest-assume python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e2ed3",
   "metadata": {},
   "source": [
    "# Prompt and Assertion for Ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f618007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For the following natural language problem description:\n",
      "\n",
      "\n",
      "def minPath(grid, k):\n",
      "    \"\"\"\n",
      "    Given a grid with N rows and N columns (N >= 2) and a positive integer k, \n",
      "    each cell of the grid contains a value. Every integer in the range [1, N * N]\n",
      "    inclusive appears exactly once on the cells of the grid.\n",
      "\n",
      "    You have to find the minimum path of length k in the grid. You can start\n",
      "    from any cell, and in each step you can move to any of the neighbor cells,\n",
      "    in other words, you can go to cells which share an edge with you current\n",
      "    cell.\n",
      "    Please note that a path of length k means visiting exactly k cells (not\n",
      "    necessarily distinct).\n",
      "    You CANNOT go off the grid.\n",
      "    A path A (of length k) is considered less than a path B (of length k) if\n",
      "    after making the ordered lists of the values on the cells that A and B go\n",
      "    through (let's call them lst_A and lst_B), lst_A is lexicographically less\n",
      "    than lst_B, in other words, there exist an integer index i (1 <= i <= k)\n",
      "    such that lst_A[i] < lst_B[i] and for any j (1 <= j < i) we have\n",
      "    lst_A[j] = lst_B[j].\n",
      "    It is guaranteed that the answer is unique.\n",
      "    Return an ordered list of the values on the cells that the minimum path go through.\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        Input: grid = [ [1,2,3], [4,5,6], [7,8,9]], k = 3\n",
      "        Output: [1, 2, 1]\n",
      "\n",
      "        Input: grid = [ [5,9,3], [4,1,6], [7,8,2]], k = 1\n",
      "        Output: [1]\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "With the method signature 'list of ints minPath(lists of lists matrix, int)'\n",
      "Please write least 5 unique formal specifications as Python assertions that describe the correct behavior of this\n",
      "method.\n",
      "Let 'res' denote the expected return value of the given method.\n",
      "Do not call the method in your assertions.\n",
      "Do not use methods with side effects such as System.out.println, file I/O, random number\n",
      "generation, or timing functions.\n",
      "Express the relationship between 'n' and 'res' using pure arithmetic and boolean logic only\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open(\"assignment3/generated_code/problem_0.txt\")\n",
    "content = file.read()\n",
    "prompt = f\"\"\"\n",
    "For the following natural language problem description:\\n\n",
    "{content}\\n\n",
    "With the method signature 'list of ints minPath(lists of lists matrix, int)'\n",
    "Please write least 5 unique formal specifications as Python assertions that describe the correct behavior of this\n",
    "method.\n",
    "Let 'res' denote the expected return value of the given method.\n",
    "Do not call the method in your assertions.\n",
    "Do not use methods with side effects such as System.out.println, file I/O, random number\n",
    "generation, or timing functions.\n",
    "Express the relationship between 'n' and 'res' using pure arithmetic and boolean logic only\n",
    "\n",
    "\"\"\"\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "147db2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 unique formal specifications as Python assertions for the `numerical_letter_grade` method, where `n` is the input list of floats and `res` is the expected return value (list of strings):\n",
      "\n",
      "1.  **Assertion 1: Output list length and element types.**\n",
      "    The output list `res` must have the same length as the input list `n`, and every element in `res` must be a string.\n",
      "    ```python\n",
      "    assert len(res) == len(n) and all(isinstance(grade, str) for grade in res)\n",
      "    ```\n",
      "\n",
      "2.  **Assertion 2: Specific mapping for the highest exact GPA.**\n",
      "    If any GPA in the input list `n` is exactly 4.0, its corresponding grade in `res` must be 'A+'.\n",
      "    ```python\n",
      "    assert all(res[i] == 'A+' for i in range(len(n)) if n[i] == 4.0)\n",
      "    ```\n",
      "\n",
      "3.  **Assertion 3: Specific mapping for a typical mid-range GPA interval.**\n",
      "    If a GPA is strictly greater than 2.7 and less than or equal to 3.0, its corresponding letter grade must be 'B'. This demonstrates correct handling of open and closed bounds.\n",
      "    ```python\n",
      "    assert all(res[i] == 'B' for i in range(len(n)) if 2.7 < n[i] <= 3.0)\n",
      "    ```\n",
      "\n",
      "4.  **Assertion 4: Specific mapping for the lowest exact GPA.**\n",
      "    If any GPA in the input list `n` is exactly 0.0, its corresponding grade in `res` must be 'E'.\n",
      "    ```python\n",
      "    assert all(res[i] == 'E' for i in range(len(n)) if n[i] == 0.0)\n",
      "    ```\n",
      "\n",
      "5.  **Assertion 5: Comprehensive check for a broad range of high-tier grades.**\n",
      "    For any GPA strictly greater than 2.7, its corresponding grade must correctly map to 'A+', 'A', 'A-', 'B+', or 'B' according to the specified rules. This assertion checks multiple interconnected grading rules.\n",
      "    ```python\n",
      "    assert all(\n",
      "        (n[i] == 4.0 and res[i] == 'A+') or\n",
      "        (3.7 < n[i] < 4.0 and res[i] == 'A') or\n",
      "        (3.3 < n[i] <= 3.7 and res[i] == 'A-') or\n",
      "        (3.0 < n[i] <= 3.3 and res[i] == 'B+') or\n",
      "        (2.7 < n[i] <= 3.0 and res[i] == 'B')\n",
      "        for i in range(len(n)) if n[i] > 2.7\n",
      "    )\n",
      "    ```\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "api_key = os.getenv(\"api_key\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "# Create the model object\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "# Print the response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60509c2",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ba247",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2f5e39",
   "metadata": {},
   "source": [
    "### output for problem 0\n",
    "Here are 5 unique formal specifications as Python assertions for the `minPath` method:\n",
    "\n",
    "Let `N` be the dimension of the grid, derived as `N = len(grid)`.\n",
    "\n",
    "1.  **The length of the returned path `res` must be exactly `k`.**\n",
    "    ```python\n",
    "    assert len(res) == k\n",
    "    ```\n",
    "\n",
    "2.  **All values in the path `res` must be valid values from the grid, i.e., integers in the range `[1, N*N]` inclusive.**\n",
    "    ```python\n",
    "    assert all(1 <= val <= N*N for val in res)\n",
    "    ```\n",
    "\n",
    "3.  **The first element of the path `res[0]` must be `1`.** This is because the problem states that every integer in `[1, N*N]` appears exactly once in the grid, meaning `1` is always the minimum value available. To achieve a lexicographically smallest path, the first element must be the smallest possible value.\n",
    "    ```python\n",
    "    assert res[0] == 1\n",
    "    ```\n",
    "\n",
    "4.  **For any two consecutive values `res[i]` and `res[i+1]` in the path, the cells containing these values in the grid must be adjacent (share an edge).**\n",
    "    ```python\n",
    "    assert all(\n",
    "        any( # Check if there exist coordinates (r1, c1) for res[i]\n",
    "            any( # Check if there exist coordinates (r2, c2) for res[i+1]\n",
    "                grid[r1][c1] == res[i] and grid[r2][c2] == res[i+1] and (\n",
    "                    abs(r1 - r2) + abs(c1 - c2) == 1\n",
    "                )\n",
    "                for r2 in range(N) for c2 in range(N)\n",
    "            )\n",
    "            for r1 in range(N) for c1 in range(N)\n",
    "        )\n",
    "        for i in range(k - 1)\n",
    "    )\n",
    "    ```\n",
    "\n",
    "5.  **If `k` is at least 2, the second element of the path `res[1]` must be the smallest value among all direct neighbors of the cell containing `1` in the grid.** This ensures the lexicographical minimality of the path's second element after the initial `1`.\n",
    "    ```python\n",
    "    assert (k < 2) or (\n",
    "        res[1] == min(\n",
    "            grid[r_neigh][c_neigh]\n",
    "            for r_1 in range(N) for c_1 in range(N) if grid[r_1][c_1] == 1\n",
    "            for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)] # Possible neighbor offsets\n",
    "            for r_neigh, c_neigh in [(r_1 + dr, c_1 + dc)]\n",
    "            if 0 <= r_neigh < N and 0 <= c_neigh < N # Check bounds for neighbor\n",
    "        )\n",
    "    )\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8033797",
   "metadata": {},
   "source": [
    "### output for problem_1\n",
    "Here are 5 unique formal specifications as Python assertions for the `numerical_letter_grade` method, where `n` is the input list of floats and `res` is the expected return value (list of strings):\n",
    "\n",
    "1.  **Assertion 1: Output list length and element types.**\n",
    "    The output list `res` must have the same length as the input list `n`, and every element in `res` must be a string.\n",
    "    ```python\n",
    "    assert len(res) == len(n) and all(isinstance(grade, str) for grade in res)\n",
    "    ```\n",
    "\n",
    "2.  **Assertion 2: Specific mapping for the highest exact GPA.**\n",
    "    If any GPA in the input list `n` is exactly 4.0, its corresponding grade in `res` must be 'A+'.\n",
    "    ```python\n",
    "    assert all(res[i] == 'A+' for i in range(len(n)) if n[i] == 4.0)\n",
    "    ```\n",
    "\n",
    "3.  **Assertion 3: Specific mapping for a typical mid-range GPA interval.**\n",
    "    If a GPA is strictly greater than 2.7 and less than or equal to 3.0, its corresponding letter grade must be 'B'. This demonstrates correct handling of open and closed bounds.\n",
    "    ```python\n",
    "    assert all(res[i] == 'B' for i in range(len(n)) if 2.7 < n[i] <= 3.0)\n",
    "    ```\n",
    "\n",
    "4.  **Assertion 4: Specific mapping for the lowest exact GPA.**\n",
    "    If any GPA in the input list `n` is exactly 0.0, its corresponding grade in `res` must be 'E'.\n",
    "    ```python\n",
    "    assert all(res[i] == 'E' for i in range(len(n)) if n[i] == 0.0)\n",
    "    ```\n",
    "\n",
    "5.  **Assertion 5: Comprehensive check for a broad range of high-tier grades.**\n",
    "    For any GPA strictly greater than 2.7, its corresponding grade must correctly map to 'A+', 'A', 'A-', 'B+', or 'B' according to the specified rules. This assertion checks multiple interconnected grading rules.\n",
    "    ```python\n",
    "    assert all(\n",
    "        (n[i] == 4.0 and res[i] == 'A+') or\n",
    "        (3.7 < n[i] < 4.0 and res[i] == 'A') or\n",
    "        (3.3 < n[i] <= 3.7 and res[i] == 'A-') or\n",
    "        (3.0 < n[i] <= 3.3 and res[i] == 'B+') or\n",
    "        (2.7 < n[i] <= 3.0 and res[i] == 'B')\n",
    "        for i in range(len(n)) if n[i] > 2.7\n",
    "    )\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e08bf",
   "metadata": {},
   "source": [
    "# Ex 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43038501",
   "metadata": {},
   "source": [
    "## Old coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85b486c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Measuring coverage for: problem_1.py\n",
      "Running test file:      test_1.py\n",
      "================================================================================\n",
      "Running command: /usr/bin/python3 -m coverage run --include=assignment3/generated_code/problem_1.py --branch -m pytest -v assignment3/tests/test_1.py\n",
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vincent/Documents/unirepo/WiSe25_26/soft_eng/520-Prompting-Debugging-and-Innovation-for-Code\n",
      "plugins: assume-2.4.3, cov-7.0.0, anyio-4.9.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "assignment3/tests/test_1.py::test_gpa_converter \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "================================================================================\n",
      "Terminal Coverage Report (shows missing lines)\n",
      "================================================================================\n",
      "Name                                      Stmts   Miss Branch BrPart  Cover   Missing\n",
      "-------------------------------------------------------------------------------------\n",
      "assignment3/generated_code/problem_1.py      29      3     26      3    89%   10, 18, 20\n",
      "-------------------------------------------------------------------------------------\n",
      "TOTAL                                        29      3     26      3    89%\n",
      "\n",
      "================================================================================\n",
      "Generating final HTML report...\n",
      "Final HTML report generated! View in browser:\n",
      "   file:///home/vincent/Documents/unirepo/WiSe25_26/soft_eng/520-Prompting-Debugging-and-Innovation-for-Code/assignment3/htmlcov_1/index.html\n",
      "\n",
      "(Could not auto-detect direct link to problem_1.py, please open index.html)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration (Set your paths here) ---\n",
    "# The specific code file you want to measure\n",
    "prob_num = 1\n",
    "CODE_FILE_BEING_TESTED = f\"assignment3/generated_code/problem_{prob_num}.py\" \n",
    "\n",
    "# The single test file you want to run\n",
    "TEST_FILE_TO_USE = f\"assignment3/tests/test_{prob_num}.py\" \n",
    "\n",
    "# The folder where the HTML report will be created\n",
    "HTML_REPORT_DIR = f\"assignment3/htmlcov_{prob_num}\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def measure_single_file_coverage(code_file_str: str, test_file_str: str, html_dir_str: str):\n",
    "    \"\"\"\n",
    "    Runs a single test file against a single source file using\n",
    "    pytest-cov and generates a terminal and HTML report.\n",
    "    \"\"\"\n",
    "    code_file = Path(code_file_str)\n",
    "    test_file = Path(test_file_str)\n",
    "\n",
    "    # --- 1. Validation ---\n",
    "    if not code_file.is_file():\n",
    "        print(f\"Error: Code file not found: {code_file.resolve()}\")\n",
    "        return\n",
    "    if not test_file.is_file():\n",
    "        print(f\"Error: Test file not found: {test_file.resolve()}\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Measuring coverage for: {code_file.name}\")\n",
    "    print(f\"Running test file:      {test_file.name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # --- 2. Erase old coverage data ---\n",
    "    subprocess.run([sys.executable, \"-m\", \"coverage\", \"erase\"], \n",
    "                   capture_output=True)\n",
    "\n",
    "    # --- 3. Run pytest with coverage ---\n",
    "    command = [\n",
    "        sys.executable,\n",
    "        \"-m\", \"coverage\", \"run\",\n",
    "        # THE FIX: Point source directly to the specific FILE, not the directory\n",
    "        f\"--include={code_file_str}\",  \n",
    "        \"--branch\",\n",
    "        \"-m\", \"pytest\",\n",
    "        \"-v\",\n",
    "        test_file_str  # Run this specific test file\n",
    "    ]\n",
    "    \n",
    "    # We don't capture output so you can see the pytest failures/output directly\n",
    "    print(f\"Running command: {' '.join(command)}\\n\")\n",
    "    subprocess.run(command)\n",
    "\n",
    "    # --- 4. Show simple terminal report ---\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Terminal Coverage Report (shows missing lines)\")\n",
    "    print(\"=\" * 80)\n",
    "    report_cmd = [\n",
    "        sys.executable,\n",
    "        \"-m\", \"coverage\", \"report\",\n",
    "        \"-m\"  # Show missing lines\n",
    "        # No need to filter by file here because we only ran coverage on one file\n",
    "    ]\n",
    "    subprocess.run(report_cmd)\n",
    "\n",
    "    # --- 5. Generate Final HTML Report ---\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Generating final HTML report...\")\n",
    "    html_command = [\n",
    "        sys.executable,\n",
    "        \"-m\", \"coverage\", \"html\",\n",
    "        \"-d\", html_dir_str\n",
    "    ]\n",
    "    subprocess.run(html_command, capture_output=True)\n",
    "    \n",
    "    report_path = Path.cwd() / html_dir_str / 'index.html'\n",
    "    \n",
    "    # Try to find the specific HTML file for the code being tested.\n",
    "    # Coverage usually replaces slashes with underscores in filenames for the HTML report.\n",
    "    # e.g. \"folder/file.py\" -> \"folder_file_py.html\"\n",
    "    flat_name = code_file_str.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\".py\", \"_py.html\")\n",
    "    file_report_path = Path.cwd() / html_dir_str / flat_name\n",
    "\n",
    "    if report_path.is_file():\n",
    "        print(f\"Final HTML report generated! View in browser:\")\n",
    "        print(f\"   {report_path.as_uri()}\")\n",
    "        \n",
    "        # Check for specific file report\n",
    "        if file_report_path.is_file():\n",
    "             print(f\"\\nDirect link to your file's report:\\n   {file_report_path.as_uri()}\")\n",
    "        else:\n",
    "             # Fallback: sometimes it just names it normally if it's in the root, \n",
    "             # or coverage naming conventions vary slightly by version.\n",
    "             print(f\"\\n(Could not auto-detect direct link to {code_file.name}, please open index.html)\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Final HTML report was not generated.\")\n",
    "        print(\"   This usually means 'coverage run' collected no data.\")\n",
    "    \n",
    "    # --- 6. Cleanup ---\n",
    "    if os.path.exists(\".coverage\"):\n",
    "        try:\n",
    "            os.remove(\".coverage\")\n",
    "        except PermissionError:\n",
    "            pass\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "# --- Run the main function ---\n",
    "if __name__ == \"__main__\":\n",
    "    measure_single_file_coverage(\n",
    "        CODE_FILE_BEING_TESTED, \n",
    "        TEST_FILE_TO_USE, \n",
    "        HTML_REPORT_DIR\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5149d",
   "metadata": {},
   "source": [
    "## Generate new tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc2094a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For the formal specification assert statements:\n",
      "\n",
      "### output for problem_1\n",
      "Here are 5 unique formal specifications as Python assertions for the `numerical_letter_grade` method, where `n` is the input list of floats and `res` is the expected return value (list of strings):\n",
      "\n",
      "\n",
      "1.  **Assertion 1: Output list length and element types.**\n",
      "   The output list `res` must have the same length as the input list `n`, and every element in `res` must be a string.\n",
      "   ```python\n",
      "   assert len(res) == len(n) and all(isinstance(grade, str) for grade in res)\n",
      "   ```\n",
      "\n",
      "\n",
      "2.  **Assertion 2: Specific mapping for the highest exact GPA.**\n",
      "   If any GPA in the input list `n` is exactly 4.0, its corresponding grade in `res` must be 'A+'.\n",
      "   ```python\n",
      "   assert all(res[i] == 'A+' for i in range(len(n)) if n[i] == 4.0)\n",
      "   ```\n",
      "\n",
      "\n",
      "3.  **Assertion 3: Specific mapping for a typical mid-range GPA interval.**\n",
      "   If a GPA is strictly greater than 2.7 and less than or equal to 3.0, its corresponding letter grade must be 'B'. This demonstrates correct handling of open and closed bounds.\n",
      "   ```python\n",
      "   assert all(res[i] == 'B' for i in range(len(n)) if 2.7 < n[i] <= 3.0)\n",
      "   ```\n",
      "\n",
      "\n",
      "4.  **Assertion 4: Specific mapping for the lowest exact GPA.**\n",
      "   If any GPA in the input list `n` is exactly 0.0, its corresponding grade in `res` must be 'E'.\n",
      "   ```python\n",
      "   assert all(res[i] == 'E' for i in range(len(n)) if n[i] == 0.0)\n",
      "   ```\n",
      "\n",
      "\n",
      "5.  **Assertion 5: Comprehensive check for a broad range of high-tier grades.**\n",
      "   For any GPA strictly greater than 2.7, its corresponding grade must correctly map to 'A+', 'A', 'A-', 'B+', or 'B' according to the specified rules. This assertion checks multiple interconnected grading rules.\n",
      "   ```python\n",
      "   assert all(\n",
      "      (n[i] == 4.0 and res[i] == 'A+') or\n",
      "      (3.7 < n[i] < 4.0 and res[i] == 'A') or\n",
      "      (3.3 < n[i] <= 3.7 and res[i] == 'A-') or\n",
      "      (3.0 < n[i] <= 3.3 and res[i] == 'B+') or\n",
      "      (2.7 < n[i] <= 3.0 and res[i] == 'B')\n",
      "      for i in range(len(n)) if n[i] > 2.7\n",
      "   )\n",
      "   ```\n",
      "\n",
      "\n",
      "Create assert statements with the following signature:\n",
      "pytest.assume(numerical_letter_grade([0, 0.7]) == ['E', 'D-'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open(\"assignment3/generated_code/spec_1.txt\")\n",
    "content = file.read()\n",
    "prompt = f\"\"\"\n",
    "For the formal specification assert statements:\\n\n",
    "{content}\\n\n",
    "Create assert statements with the following signature:\n",
    "pytest.assume(numerical_letter_grade([0, 0.7]) == ['E', 'D-'])\n",
    "\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74167e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 `pytest.assume` statements, each providing a concrete test case that demonstrates the properties described by the original formal specifications.\n",
      "\n",
      "```python\n",
      "import pytest\n",
      "\n",
      "# Assume numerical_letter_grade is defined elsewhere, e.g.:\n",
      "def numerical_letter_grade(n: list[float]) -> list[str]:\n",
      "    \"\"\"\n",
      "    Converts a list of numerical GPAs to a list of letter grades.\n",
      "    \"\"\"\n",
      "    grades = []\n",
      "    for gpa in n:\n",
      "        if gpa == 4.0:\n",
      "            grades.append('A+')\n",
      "        elif 3.7 < gpa < 4.0:\n",
      "            grades.append('A')\n",
      "        elif 3.3 < gpa <= 3.7:\n",
      "            grades.append('A-')\n",
      "        elif 3.0 < gpa <= 3.3:\n",
      "            grades.append('B+')\n",
      "        elif 2.7 < gpa <= 3.0:\n",
      "            grades.append('B')\n",
      "        elif 2.3 < gpa <= 2.7:\n",
      "            grades.append('C+')\n",
      "        elif 2.0 < gpa <= 2.3:\n",
      "            grades.append('C')\n",
      "        elif 1.7 < gpa <= 2.0:\n",
      "            grades.append('C-')\n",
      "        elif 1.3 < gpa <= 1.7:\n",
      "            grades.append('D+')\n",
      "        elif 1.0 < gpa <= 1.3:\n",
      "            grades.append('D')\n",
      "        elif 0.7 < gpa <= 1.0:\n",
      "            grades.append('D-')\n",
      "        elif gpa == 0.0:\n",
      "            grades.append('E') # As per assertion 4\n",
      "        else: # Covers values like 0.1 to 0.7 which are F in many systems, but for 0 it is E\n",
      "            grades.append('F') # Default for grades below D-, assuming 'E' is only for 0.0 based on Assertion 4\n",
      "    return grades\n",
      "\n",
      "\n",
      "# 1. Assertion 1: Output list length and element types.\n",
      "# Demonstrates that the output has the same length as input and all elements are strings.\n",
      "pytest.assume(numerical_letter_grade([0.0, 4.0, 2.5]) == ['E', 'A+', 'C+'])\n",
      "\n",
      "# 2. Assertion 2: Specific mapping for the highest exact GPA.\n",
      "# Demonstrates that a GPA of exactly 4.0 maps to 'A+'.\n",
      "pytest.assume(numerical_letter_grade([4.0]) == ['A+'])\n",
      "\n",
      "# 3. Assertion 3: Specific mapping for a typical mid-range GPA interval.\n",
      "# Demonstrates that GPAs strictly greater than 2.7 and less than or equal to 3.0 map to 'B'.\n",
      "pytest.assume(numerical_letter_grade([2.8, 3.0]) == ['B', 'B'])\n",
      "\n",
      "# 4. Assertion 4: Specific mapping for the lowest exact GPA.\n",
      "# Demonstrates that a GPA of exactly 0.0 maps to 'E'.\n",
      "pytest.assume(numerical_letter_grade([0.0]) == ['E'])\n",
      "\n",
      "# 5. Assertion 5: Comprehensive check for a broad range of high-tier grades.\n",
      "# Demonstrates correct mappings for GPAs > 2.7 across multiple defined intervals.\n",
      "pytest.assume(numerical_letter_grade([4.0, 3.8, 3.4, 3.1, 2.9]) == ['A+', 'A', 'A-', 'B+', 'B'])\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "api_key = os.getenv(\"api_key\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "# Create the model object\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "# Print the response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1365bfd2",
   "metadata": {},
   "source": [
    "## New coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20e2190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Measuring coverage for: problem_1.py\n",
      "Running test file:      test_1_spec_assertions.py\n",
      "================================================================================\n",
      "Running command: /usr/bin/python3 -m coverage run --include=assignment3/generated_code/problem_1.py --branch -m pytest -v assignment3/tests/test_1_spec_assertions.py\n",
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vincent/Documents/unirepo/WiSe25_26/soft_eng/520-Prompting-Debugging-and-Innovation-for-Code\n",
      "plugins: assume-2.4.3, cov-7.0.0, anyio-4.9.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "assignment3/tests/test_1_spec_assertions.py::test_gpa_converter \u001b[31mFAILED\u001b[0m\u001b[31m   [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_gpa_converter ______________________________\u001b[0m\n",
      "\n",
      "tp = <class 'pytest_assume.plugin.FailedAssumption'>, value = None, tb = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mreraise\u001b[39;49;00m(tp, value, tb=\u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m value \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                value = tp()\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m value.__traceback__ \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m tb:\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[94mraise\u001b[39;49;00m value.with_traceback(tb)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               pytest_assume.plugin.FailedAssumption: \u001b[0m\n",
      "\u001b[1m\u001b[31mE               1 Failed Assumptions:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               \u001b[0m\n",
      "\u001b[1m\u001b[31mE               assignment3/tests/test_1_spec_assertions.py:36: AssumptionFailure\u001b[0m\n",
      "\u001b[1m\u001b[31mE               >>\tpytest.assume(numerical_letter_grade([0.0, 4.0, 2.5]) == ['E', 'A+', 'C+'])\u001b[0m\n",
      "\u001b[1m\u001b[31mE               AssertionError: assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/usr/lib/python3/dist-packages/six.py\u001b[0m:718: FailedAssumption\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "assignment3/tests/test_1_spec_assertions.py::test_gpa_converter\n",
      "  /home/vincent/.local/lib/python3.10/site-packages/_pytest/runner.py:246: PluggyTeardownRaisedWarning: A plugin raised an exception during an old-style hookwrapper teardown.\n",
      "  Plugin: assume, Hook: pytest_runtest_call\n",
      "  FailedAssumption: \n",
      "  1 Failed Assumptions:\n",
      "  \n",
      "  assignment3/tests/test_1_spec_assertions.py:36: AssumptionFailure\n",
      "  >>\tpytest.assume(numerical_letter_grade([0.0, 4.0, 2.5]) == ['E', 'A+', 'C+'])\n",
      "  AssertionError: assert False\n",
      "  \n",
      "  \n",
      "  For more information see https://pluggy.readthedocs.io/en/stable/api_reference.html#pluggy.PluggyTeardownRaisedWarning\n",
      "    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m assignment3/tests/test_1_spec_assertions.py::\u001b[1mtest_gpa_converter\u001b[0m - pytest_assume.plugin.FailedAssumption: \n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 0.16s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "\n",
      "================================================================================\n",
      "Terminal Coverage Report (shows missing lines)\n",
      "================================================================================\n",
      "Name                                      Stmts   Miss Branch BrPart  Cover   Missing\n",
      "-------------------------------------------------------------------------------------\n",
      "assignment3/generated_code/problem_1.py      29      1     26      1    96%   20\n",
      "-------------------------------------------------------------------------------------\n",
      "TOTAL                                        29      1     26      1    96%\n",
      "\n",
      "================================================================================\n",
      "Generating final HTML report...\n",
      "Final HTML report generated! View in browser:\n",
      "   file:///home/vincent/Documents/unirepo/WiSe25_26/soft_eng/520-Prompting-Debugging-and-Innovation-for-Code/assignment3/htmlcov_1_spec/index.html\n",
      "\n",
      "(Could not auto-detect direct link to problem_1.py, please open index.html)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration (Set your paths here) ---\n",
    "# The specific code file you want to measure\n",
    "prob_num = 1\n",
    "CODE_FILE_BEING_TESTED = f\"assignment3/generated_code/problem_{prob_num}.py\" \n",
    "\n",
    "# The single test file you want to run\n",
    "TEST_FILE_TO_USE = f\"assignment3/tests/test_{prob_num}_spec_assertions.py\" \n",
    "\n",
    "# The folder where the HTML report will be created\n",
    "HTML_REPORT_DIR = f\"assignment3/htmlcov_{prob_num}_spec\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def measure_single_file_coverage(code_file_str: str, test_file_str: str, html_dir_str: str):\n",
    "    \"\"\"\n",
    "    Runs a single test file against a single source file using\n",
    "    pytest-cov and generates a terminal and HTML report.\n",
    "    \"\"\"\n",
    "    code_file = Path(code_file_str)\n",
    "    test_file = Path(test_file_str)\n",
    "\n",
    "    # --- 1. Validation ---\n",
    "    if not code_file.is_file():\n",
    "        print(f\"Error: Code file not found: {code_file.resolve()}\")\n",
    "        return\n",
    "    if not test_file.is_file():\n",
    "        print(f\"Error: Test file not found: {test_file.resolve()}\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Measuring coverage for: {code_file.name}\")\n",
    "    print(f\"Running test file:      {test_file.name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # --- 2. Erase old coverage data ---\n",
    "    subprocess.run([sys.executable, \"-m\", \"coverage\", \"erase\"], \n",
    "                   capture_output=True)\n",
    "\n",
    "    # --- 3. Run pytest with coverage ---\n",
    "    command = [\n",
    "        sys.executable,\n",
    "        \"-m\", \"coverage\", \"run\",\n",
    "        # THE FIX: Point source directly to the specific FILE, not the directory\n",
    "        f\"--include={code_file_str}\",  \n",
    "        \"--branch\",\n",
    "        \"-m\", \"pytest\",\n",
    "        \"-v\",\n",
    "        test_file_str  # Run this specific test file\n",
    "    ]\n",
    "    \n",
    "    # We don't capture output so you can see the pytest failures/output directly\n",
    "    print(f\"Running command: {' '.join(command)}\\n\")\n",
    "    subprocess.run(command)\n",
    "\n",
    "    # --- 4. Show simple terminal report ---\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Terminal Coverage Report (shows missing lines)\")\n",
    "    print(\"=\" * 80)\n",
    "    report_cmd = [\n",
    "        sys.executable,\n",
    "        \"-m\", \"coverage\", \"report\",\n",
    "        \"-m\"  # Show missing lines\n",
    "        # No need to filter by file here because we only ran coverage on one file\n",
    "    ]\n",
    "    subprocess.run(report_cmd)\n",
    "\n",
    "    # --- 5. Generate Final HTML Report ---\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Generating final HTML report...\")\n",
    "    html_command = [\n",
    "        sys.executable,\n",
    "        \"-m\", \"coverage\", \"html\",\n",
    "        \"-d\", html_dir_str\n",
    "    ]\n",
    "    subprocess.run(html_command, capture_output=True)\n",
    "    \n",
    "    report_path = Path.cwd() / html_dir_str / 'index.html'\n",
    "    \n",
    "    # Try to find the specific HTML file for the code being tested.\n",
    "    # Coverage usually replaces slashes with underscores in filenames for the HTML report.\n",
    "    # e.g. \"folder/file.py\" -> \"folder_file_py.html\"\n",
    "    flat_name = code_file_str.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\".py\", \"_py.html\")\n",
    "    file_report_path = Path.cwd() / html_dir_str / flat_name\n",
    "\n",
    "    if report_path.is_file():\n",
    "        print(f\"Final HTML report generated! View in browser:\")\n",
    "        print(f\"   {report_path.as_uri()}\")\n",
    "        \n",
    "        # Check for specific file report\n",
    "        if file_report_path.is_file():\n",
    "             print(f\"\\nDirect link to your file's report:\\n   {file_report_path.as_uri()}\")\n",
    "        else:\n",
    "             # Fallback: sometimes it just names it normally if it's in the root, \n",
    "             # or coverage naming conventions vary slightly by version.\n",
    "             print(f\"\\n(Could not auto-detect direct link to {code_file.name}, please open index.html)\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Final HTML report was not generated.\")\n",
    "        print(\"   This usually means 'coverage run' collected no data.\")\n",
    "    \n",
    "    # --- 6. Cleanup ---\n",
    "    if os.path.exists(\".coverage\"):\n",
    "        try:\n",
    "            os.remove(\".coverage\")\n",
    "        except PermissionError:\n",
    "            pass\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "# --- Run the main function ---\n",
    "if __name__ == \"__main__\":\n",
    "    measure_single_file_coverage(\n",
    "        CODE_FILE_BEING_TESTED, \n",
    "        TEST_FILE_TO_USE, \n",
    "        HTML_REPORT_DIR\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
